{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AnchorModel.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyN8CszPBrlG50aY2KDHSRWU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"l381TyBduH0W","colab_type":"code","outputId":"40b328ab-c249-4def-c29e-04ab99df72dd","executionInfo":{"status":"ok","timestamp":1588513782018,"user_tz":300,"elapsed":66375,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["#Mounting Google Drive to Colab Pro\n","!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n","!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n","!apt-get update -qq 2>&1 > /dev/null\n","!apt-get -y install -qq google-drive-ocamlfuse fuse\n","from google.colab import auth\n","auth.authenticate_user()\n","from oauth2client.client import GoogleCredentials\n","creds = GoogleCredentials.get_application_default()\n","import getpass\n","!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n","vcode = getpass.getpass()\n","!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["E: Package 'python-software-properties' has no installation candidate\n","Selecting previously unselected package google-drive-ocamlfuse.\n","(Reading database ... 144568 files and directories currently installed.)\n","Preparing to unpack .../google-drive-ocamlfuse_0.7.21-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n","Unpacking google-drive-ocamlfuse (0.7.21-0ubuntu1~ubuntu18.04.1) ...\n","Setting up google-drive-ocamlfuse (0.7.21-0ubuntu1~ubuntu18.04.1) ...\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","··········\n","Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n","Please enter the verification code: Access token retrieved correctly.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OWnBD3JauOSd","colab_type":"code","colab":{}},"source":["#Mounting Google Drive to Colab Pro\n","!mkdir -p drive\n","!google-drive-ocamlfuse drive"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"edrBlBe9ufln","colab_type":"code","outputId":"24367b32-10b5-47f0-b2e6-d4066a77259f","executionInfo":{"status":"ok","timestamp":1588513796912,"user_tz":300,"elapsed":2596,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["#Testing access to Google drive\n","!ls /content/drive/StatNLP/FinalProject/"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" ADVERSARIA-ATTACK-GENERATOR-Dheeraj.ipynb\t     'Data Sets'\n"," AnchorTags_and_Adversarial-Attacks-Generator.ipynb  'Model Files'\n"," Blackbox.ipynb\t\t\t\t\t      README.md\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sZYQYJFLuzjP","colab_type":"text"},"source":["## **Master Variables**"]},{"cell_type":"code","metadata":{"id":"mQ-k5lp-ujWR","colab_type":"code","colab":{}},"source":["#Data Storage Parameters\n","dataset_dir = \"/content/drive/StatNLP/FinalProject/Data Sets/asap-aes\"\n","adversarial_dir = \"/content/drive/StatNLP/FinalProject/Data Sets/adversarial_asap\"\n","model_save_dir = \"/content/drive/StatNLP/FinalProject/Model Files\"\n","selected_essay_id = 2\n","training_set_file = dataset_dir+\"/training_set_rel3.xls\" \n","\n","###Test sets:\n","test_set_file = dataset_dir+\"/valid_set.xls\"\n","test_set_scores_file = dataset_dir+\"/valid_sample_submission_5_column.csv\"\n","\n","#Data Embedding Parameters\n","# Take First X words from each essay, abandon rest\n","max_len = 1118 #longest essay \n","\n","# Word Dimensionality - consider the top 15,000 words in the dataset\n","max_words = 20000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m6vWoJ3tuteZ","colab_type":"code","colab":{}},"source":["\n","def make_prediction(modelname,sampess):\n","    sample_prediction = modelname.predict(test_set_essays_emb[sampess:sampess+1])\n","    return sample_prediction\n","  \n","def calculate_score(prediction):\n","    score = {}\n","    score[1]=prediction[0,0]\n","    score[2]=prediction[0,1]\n","    score[3]=prediction[0,2]    \n","    score[4]=prediction[0,3]  \n","    score[5]=prediction[0,4]    \n","    score[6]=prediction[0,5]    \n","    calculate_score = max(score, key=score.get)\n","    return(calculate_score)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkRZie3YvDBP","colab_type":"text"},"source":["# **Loading Packages and Dependencies**"]},{"cell_type":"code","metadata":{"id":"SmqLT4aCvBTW","colab_type":"code","outputId":"a5dad08e-ddff-4855-ca78-60b80066c2ab","executionInfo":{"status":"ok","timestamp":1588513823055,"user_tz":300,"elapsed":2138,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["#data loading\n","import os\n","\n","# python modules\n","from argparse import Namespace\n","from collections import Counter\n","import json\n","import re\n","import string\n","import statistics\n","\n","####data manipulation####\n","import numpy as np\n","from numpy.random import shuffle \n","import pandas as pd\n","\n","####word2vec encoding####\n","import gensim\n","\n","####data visualization####\n","%matplotlib notebook\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","from matplotlib.ticker import PercentFormatter\n","%matplotlib inline\n","plt.style.use('ggplot')\n","\n","####CNN tools####\n","\n","#keras\n","import keras\n","from keras import layers\n","from keras import models\n","from keras.models import Sequential\n","from keras.layers import Embedding, Flatten, Dense\n","from keras.models import load_model\n","from keras import regularizers\n","from keras import metrics\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"TxXjlE_KvX8m","colab_type":"text"},"source":["## ** Load and Clean Dataset**"]},{"cell_type":"code","metadata":{"id":"HXA-oxvJvVJq","colab_type":"code","colab":{}},"source":["\n","#from google.colab import drive\n","#drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SMhMTc47vuOI","colab_type":"code","outputId":"aad91ed9-411a-48c5-a509-8a5664a29b00","executionInfo":{"status":"ok","timestamp":1588513834752,"user_tz":300,"elapsed":4039,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":208}},"source":["#verify data paths\n","print(training_set_file)\n","print(test_set_file)\n","\n","#load excel into dataframe\n","raw_training_set = pd.read_excel(training_set_file, sheet_name='training_set')\n","test_set = pd.read_excel(test_set_file, sheet_name='Sheet1')\n","test_set_scores = pd.read_csv(test_set_scores_file)\n","\n","print(\"\\nEntire Corpus for ASAP:\")\n","print(\"Training Set:\",raw_training_set.shape)\n","#print(\"Validation:\",valid_set.shape)\n","print(\"Test Set:\",test_set.shape,\"\\n\")\n","\n","#filter data by essay set\n","essay_fltr =  raw_training_set['essay_set']== selected_essay_id\n","training_set = raw_training_set[essay_fltr]\n","\n","essay_fltr =  test_set['essay_set']== selected_essay_id\n","test_set = test_set[essay_fltr]\n","\n","essay_fltr =  test_set_scores['essay_set']== selected_essay_id\n","test_set_scores = test_set_scores[essay_fltr]\n","\n","#remove empty n/a cells\n","training_set = training_set.drop(['rater3_domain1','rater1_trait1','rater1_trait2','rater1_trait3','rater1_trait4','rater1_trait5','rater1_trait6','rater2_trait1','rater2_trait2','rater2_trait3','rater2_trait4','rater2_trait5','rater2_trait6','rater3_trait1','rater3_trait2','rater3_trait3','rater3_trait4','rater3_trait5','rater3_trait6'], axis=1)    \n","test_set = test_set.drop(['domain2_predictionid'], axis=1)    \n","\n","training_set_top = training_set.head()\n","#print(training_set_top)\n","test_set_top = test_set.head()\n","#print(test_set_top)\n","\n","#3 sets, training, validation and testing\n","\n","print(\"Selected Essay Set #%s Corpus:\" % selected_essay_id)\n","print(\"Training Set:\",training_set.shape)\n","print(\"Test Set:\",test_set.shape)\n","print(\"Total Data Set:\", training_set.shape[0]+test_set.shape[0])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/StatNLP/FinalProject/Data Sets/asap-aes/training_set_rel3.xls\n","/content/drive/StatNLP/FinalProject/Data Sets/asap-aes/valid_set.xls\n","\n","Entire Corpus for ASAP:\n","Training Set: (1800, 28)\n","Test Set: (600, 5) \n","\n","Selected Essay Set #2 Corpus:\n","Training Set: (1800, 9)\n","Test Set: (600, 4)\n","Total Data Set: 2400\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1MBzjRPwf0cG","colab_type":"text"},"source":["Splitting into Essays and Labels"]},{"cell_type":"code","metadata":{"id":"YrxSs_vdvmry","colab_type":"code","colab":{}},"source":["#extract essays and convert to NumPy for Keras\n","training_set_essays = training_set['essay']\n","training_set_essays = training_set_essays.values\n","test_set_essays = test_set['essay']\n","test_set_essays = test_set_essays.values\n","\n","#extract scores and convert to NumPy for Keras\n","training_set_dom1scores = training_set['domain1_score']\n","training_set_dom1scores = training_set_dom1scores.values\n","\n","#extract domain#1 predicted scores\n","#data cleaning due to strange score input shape\n","test_set_dom1scores = []\n","for i in (range(test_set_scores.shape[0])):\n","    if (i % 2) == 0: #print every other cell, since second cell is domain#2\n","        asdf = test_set_scores['predicted_score'].values[i]\n","        i_score_no = float(asdf)\n","        #print(asdf)\n","        #test_set_dom1scores = test_set_dom1scores.append({'predicted_score': asdf}, ignore_index=True)\n","        test_set_dom1scores.append(i_score_no)\n","#convert to NumPy Array\n","test_set_dom1scores = np.asarray(test_set_dom1scores)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bRbNy3Zwf7Nt","colab_type":"text"},"source":["## **Encoding Essays**"]},{"cell_type":"markdown","metadata":{"id":"aGZ3XhmBgAhR","colab_type":"text"},"source":["Tokenization and word indexing"]},{"cell_type":"code","metadata":{"id":"WGPO0QoNf4r3","colab_type":"code","outputId":"e7b3aa09-c832-4201-df69-8cb9cca0b01f","executionInfo":{"status":"ok","timestamp":1588513840894,"user_tz":300,"elapsed":1201,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["# Vectorize the Essays\n","\n","#TEMPORARILY COMBINE TRAIN AND TEST TO SIMPLIFY EMBEDDING PROCESS\n","#single embedding process, max token index\n","lengthmark = len(training_set_essays)\n","combined_essays = np.append(training_set_essays,test_set_essays)\n","\n","# Tokenize the data \n","tokenizer = Tokenizer(num_words=max_words)\n","tokenizer.fit_on_texts(combined_essays)\n","sequences = tokenizer.texts_to_sequences(combined_essays)\n","\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))\n","\n","# Pad sequences that are shorter than others\n","combined_data_pen = pad_sequences(sequences, maxlen=max_len)\n","\n","#SPLIT TRAINING AND TEST SETS BACK\n","train_data_pen = combined_data_pen[:lengthmark]\n","test_data_pen = combined_data_pen[lengthmark:]\n","\n","# Load the label\n","print('Shape of Testing data tensor:', test_data_pen.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Found 17023 unique tokens.\n","Shape of Testing data tensor: (600, 1118)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gOTAdv9SgNfh","colab_type":"text"},"source":["One-hot encoding of essay scores"]},{"cell_type":"code","metadata":{"id":"J2-Y4ikIgHFz","colab_type":"code","outputId":"e9340e97-d62f-42ec-d763-512a93b1d5ab","executionInfo":{"status":"ok","timestamp":1588513843817,"user_tz":300,"elapsed":261,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["train_labels_pen = np.zeros((0, 6))\n","\n","#Scores to Dummy Variable Conversion\n","#Training (and Validation) Set\n","for item in training_set_dom1scores:\n","      if item==1:\n","          train_labels_pen = np.append(train_labels_pen, [[1,0,0,0,0,0]],axis = 0)\n","      elif item==2:\n","          train_labels_pen = np.append(train_labels_pen, [[0,1,0,0,0,0]],axis = 0)        \n","      elif item==3:\n","          train_labels_pen = np.append(train_labels_pen, [[0,0,1,0,0,0]],axis = 0)        \n","      elif item==4:\n","          train_labels_pen = np.append(train_labels_pen, [[0,0,0,1,0,0]],axis = 0)        \n","      elif item==5:\n","          train_labels_pen = np.append(train_labels_pen, [[0,0,0,0,1,0]],axis = 0)        \n","      else:\n","          train_labels_pen = np.append(train_labels_pen, [[0,0,0,0,0,1]],axis = 0)   \n","\n","test_labels_pen = np.zeros((0, 6))\n","\n","#Scores to Dummy Variable Conversion\n","#Testing Set\n","for item in test_set_dom1scores:\n","      if item==1:\n","          test_labels_pen = np.append(test_labels_pen, [[1,0,0,0,0,0]],axis = 0)\n","      elif item==2:\n","          test_labels_pen = np.append(test_labels_pen, [[0,1,0,0,0,0]],axis = 0)        \n","      elif item==3:\n","          test_labels_pen = np.append(test_labels_pen, [[0,0,1,0,0,0]],axis = 0)        \n","      elif item==4:\n","          test_labels_pen = np.append(test_labels_pen, [[0,0,0,1,0,0]],axis = 0)        \n","      elif item==5:\n","          test_labels_pen = np.append(test_labels_pen, [[0,0,0,0,1,0]],axis = 0)        \n","      else:\n","          test_labels_pen = np.append(test_labels_pen, [[0,0,0,0,0,1]],axis = 0)   \n","          \n","print(\"Test Labels Shape:\" ,test_labels_pen.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Labels Shape: (600, 6)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7aY6JJ2UgT2O","colab_type":"code","outputId":"24902aba-af54-4e05-eecb-fcb424da5b20","executionInfo":{"status":"ok","timestamp":1588513844984,"user_tz":300,"elapsed":195,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["#TEST SET IS LEFT ALONE\n","test_split = 0.80\n","val_set_essays = training_set_essays \n","val_set_dom1scores = training_set_dom1scores\n","\n","#split coded scores\n","set_split_test = int((len(train_data_pen))*test_split)\n","training_set_essays_emb, val_set_essays_emb = train_data_pen[:set_split_test], train_data_pen[set_split_test:]\n","training_set_dom1scores_emb, val_set_dom1scores_emb = train_labels_pen[:set_split_test], train_labels_pen[set_split_test:]\n","#split the unencoded scores\n","training_set_dom1scores, val_set_dom1scores = training_set_dom1scores[:set_split_test], training_set_dom1scores[set_split_test:]\n","\n","test_set_essays_emb = test_data_pen\n","test_set_dom1scores_emb = test_labels_pen\n","\n","print(\"\\nTest Set Essays and matching Scores:\")\n","print(\"Shape: \",test_set_essays_emb.shape, test_set_dom1scores_emb.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["\n","Test Set Essays and matching Scores:\n","Shape:  (600, 1118) (600, 6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JL59GjRphJC1","colab_type":"text"},"source":["## **Loadind Blackbox model**"]},{"cell_type":"code","metadata":{"id":"do_59K_Egco8","colab_type":"code","colab":{}},"source":["test_model_black_box = load_model(model_save_dir+'/D1_76_BLACKBOX_CNN.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cGoxWfEFhSg2","colab_type":"text"},"source":["## **Anchor Identification Model**"]},{"cell_type":"code","metadata":{"id":"ZYeohsOuhOos","colab_type":"code","outputId":"ba159cb9-e8ed-4b74-c154-c6c2a1379125","executionInfo":{"status":"ok","timestamp":1588514055230,"user_tz":300,"elapsed":193309,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","!pip install anchor_exp\n","!pip install -q spacy && python -m spacy download en_core_web_lg && python -m spacy link en_core_web_lg enlg\n","\n","from anchor import anchor_text\n","\n","import spacy\n","spacy_nlp = spacy.load('enlg')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting anchor_exp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/0a/e8bde54744085a9d3ddbb4a4a8531386b24dcab0c61d6eec7bdec2032194/anchor_exp-0.0.1.0.tar.gz (428kB)\n","\r\u001b[K     |▊                               | 10kB 23.6MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30kB 3.5MB/s eta 0:00:01\r\u001b[K     |███                             | 40kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51kB 3.2MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61kB 3.8MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 81kB 3.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92kB 4.3MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 133kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 143kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 153kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 163kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 194kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 204kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 225kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 235kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 245kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 256kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 266kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 276kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 286kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 296kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 307kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 317kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 327kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 337kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 348kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 358kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 368kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 378kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 389kB 4.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 399kB 4.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 409kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 419kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430kB 4.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (1.18.3)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (1.4.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (2.2.4)\n","Collecting lime\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/ee/4aaac4cd79f16329746495aca96f8c35f278b5c774eff3358eaa21e1cbf3/lime-0.2.0.0.tar.gz (274kB)\n","\u001b[K     |████████████████████████████████| 276kB 10.7MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.6/dist-packages (from anchor_exp) (0.22.2.post1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (4.38.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (46.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (2.23.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (3.0.2)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.1.3)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (1.0.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (2.0.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy->anchor_exp) (0.6.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime->anchor_exp) (3.2.1)\n","Collecting pillow==5.4.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/5e/e91792f198bbc5a0d7d3055ad552bc4062942d27eaf75c3e2783cf64eae5/Pillow-5.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K     |████████████████████████████████| 2.0MB 9.4MB/s \n","\u001b[?25hRequirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime->anchor_exp) (0.16.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22->anchor_exp) (0.14.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (2020.4.5.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy->anchor_exp) (2.9)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (1.6.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (2.8.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (0.10.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime->anchor_exp) (1.2.0)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (2.4.1)\n","Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (1.1.1)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime->anchor_exp) (2.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy->anchor_exp) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->lime->anchor_exp) (1.12.0)\n","Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime->anchor_exp) (4.4.2)\n","Building wheels for collected packages: anchor-exp, lime\n","  Building wheel for anchor-exp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for anchor-exp: filename=anchor_exp-0.0.1.0-cp36-none-any.whl size=434742 sha256=69740d97daae7669675a1fd6b0ec84d0e5af81cd858256ac4289f99d19305e0d\n","  Stored in directory: /root/.cache/pip/wheels/ff/b7/cd/7bfb36f4a01ff6b1509cd3432f8b208f455d232cee1079e309\n","  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for lime: filename=lime-0.2.0.0-cp36-none-any.whl size=284181 sha256=3ff30060d0d3fa42f0ad4412d566446da5a8009a6e737532bbbc55d39e6934d8\n","  Stored in directory: /root/.cache/pip/wheels/22/f2/ec/e5ebd07348b2b1ac722e91c2f549fcc220f7d5f25497a61232\n","Successfully built anchor-exp lime\n","\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Installing collected packages: pillow, lime, anchor-exp\n","  Found existing installation: Pillow 7.0.0\n","    Uninstalling Pillow-7.0.0:\n","      Successfully uninstalled Pillow-7.0.0\n","Successfully installed anchor-exp-0.0.1.0 lime-0.2.0.0 pillow-5.4.1\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting en_core_web_lg==2.2.5\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.2.5/en_core_web_lg-2.2.5.tar.gz (827.9MB)\n","\u001b[K     |████████████████████████████████| 827.9MB 2.0MB/s \n","\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_lg==2.2.5) (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.0.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (4.38.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.6.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.18.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (0.4.1)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (2.23.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (7.4.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (46.1.3)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_lg==2.2.5) (1.1.3)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.6.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_lg==2.2.5) (2020.4.5.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_lg==2.2.5) (3.1.0)\n","Building wheels for collected packages: en-core-web-lg\n","  Building wheel for en-core-web-lg (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for en-core-web-lg: filename=en_core_web_lg-2.2.5-cp36-none-any.whl size=829180944 sha256=fd712a060ae34b892b11556def94c97e9977d5b554fa8544c3a8cef6b33e849c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-j5iavqye/wheels/2a/c1/a6/fc7a877b1efca9bc6a089d6f506f16d3868408f9ff89f8dbfc\n","Successfully built en-core-web-lg\n","Installing collected packages: en-core-web-lg\n","Successfully installed en-core-web-lg-2.2.5\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_lg')\n","\u001b[38;5;2m✔ Linking successful\u001b[0m\n","/usr/local/lib/python3.6/dist-packages/en_core_web_lg -->\n","/usr/local/lib/python3.6/dist-packages/spacy/data/enlg\n","You can now load the model via spacy.load('enlg')\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"K-badZABho7J","colab_type":"code","outputId":"22d71436-32cc-428c-9a63-aac9c00296ba","executionInfo":{"status":"error","timestamp":1587539870170,"user_tz":300,"elapsed":980203,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["sample_ids = [0]\n","\n","for idx in sample_ids:\n","    print('Index: %d, Feature: %s' % (idx, test_set_essays[idx]))\n","    print('True Score: %s' % (test_set_dom1scores[idx]))\n","    estimatedscore=[calculate_score(make_prediction(test_model_black_box, idx))]\n","    def estimator(estimatedscore):\n","      estimator = np.asarray(estimatedscore)\n","      return estimator\n","\n","    #classifier_fn([text])[0]\n","\n","    explainer = anchor_text.AnchorText(spacy_nlp, [1,2,3,4,5,6], use_unk_distribution=True)\n","    exp = explainer.explain_instance(test_set_essays[idx], estimator, threshold=0.95, use_proba=True, batch_size=30)\n","\n","    max_pred = 2\n","    print('Key Signal from Anchors: %s' % (' AND '.join(exp.names())))\n","    print(exp.features())\n","    print('Precision: %.2f' % exp.precision())\n","    print()\n","\n","    #exp.show_in_notebook()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Index: 0, Feature: Do u believe there are books, music, magizines, and movies in are libaries? These meterials could consist of nude pictures and so on. Most parents do not want to see there kids getting a hold of this type of material. You maybe think 'what could  we do about this things, or least come to a compermise to make almost everyone happy. I have so reasons why we should put this type of material away from kids sight and reach.     First of all, the books and magizines that have any nude in them, should have their own section and, be labled aduts only. I know this might be alot of work but it is needed, so kids aren't aloud to go in ths section. This will help ruduce what little kids see. There are also books and magizines that are offensive to children of a different race. These books should be removed from the shelves also because they hert peoples feelings, and this might make customers leave your libary. I am not saying ouhae to take ever single book of your selves, but a least reduce the number of books of your selves out of kids eyes mostly and some adults if they are offended.          Next thing that comes to mind is the music on compacked discs, or cd's which ever you perfere.  In this day and age there are alot of musicians out there that have alot of bad words in their songs. These songs; are songs parents dont like there children to listen to, because parents are afraid of their children here these words and repeating them at home. These type of cd's should be taken of shelves where kids are, and put in a section also labled adults only. I know this wont stop kids from hearing these words and say them, but at least the parents can blame you for their child saying these words, because you did the smart thing and put these cd's out of kids reach. There are songs that are ofensive to people but i know you cant go throught every song and take them off the shelves, but a least take the cd's off the shelve that you know are offensive to people.     The last that comes to mind that should be taken off the shelves that are material children shouldnt see is the movies. There are movies out there that are to graphic, to vilent, and to much nudity that kids dont need to see. These type of movies need to also get a section that is for adults only. Then you have the movies that need to just be taking off the shelve because they are to offensive to some viewers. I know that you @MONTH1 say but if it is offensive to them, they dont have to get it. This maybe true but a least move the to a section that those people that get offended dont go.     Those were my thoughts on what should be done with material thats to offensive and not aprobriet for eyes or minds.These might be a little extreme but i hope you at least take some of these things into consideration. You cant please all but you can try. Thanks for reading this and i hope it as changed your thoughts on material thats bad for kids to see\n","True Score: 4.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3CHtDV9iiTu5","colab_type":"code","colab":{}},"source":["\n","#using a real essay\n","\n","sample_ids = [0]\n","\n","for idx in sample_ids:\n","    print('Index: %d, Feature: %s' % (idx, training_set_essays[idx]))\n","    print('True Score: %s' % (training_set_dom1scores[idx]))\n","    estimatedscore=[calculate_score(make_prediction(test_model_black_box, idx))]\n","    def estimator(estimatedscore):\n","      estimator = np.asarray(estimatedscore)\n","      return estimator\n","\n","    #classifier_fn([text])[0]\n","\n","    explainer = anchor_text.AnchorText(spacy_nlp, [1,2,3,4,5,6], use_unk_distribution=True)\n","    exp = explainer.explain_instance(training_set_essays[idx], estimator, threshold=0.8, use_proba=True, batch_size=30)\n","\n","    max_pred = 2\n","    print('Key Signal from Anchors: %s' % (' AND '.join(exp.names())))\n","    print('Precision: %.2f' % exp.precision())\n","    print()\n","\n","    #exp.show_in_notebook()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M0Xh5Tv8ic7W","colab_type":"code","outputId":"09bcb578-fa92-4419-e826-6a5884f7dc3b","executionInfo":{"status":"ok","timestamp":1587539907357,"user_tz":300,"elapsed":1713,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":306}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Wed Apr 22 07:18:26 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P0    33W / 250W |   1579MiB / 16280MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","|    0      2019      C   google-drive-ocamlfuse                      1569MiB |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AwDQa-2vpkH0","colab_type":"code","outputId":"e4bfa7d6-321e-41dc-9cbd-cc1e10974548","executionInfo":{"status":"ok","timestamp":1587540455883,"user_tz":300,"elapsed":298,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["adversarial_dir = \"/content/drive/StatNLP/FinalProject/Data Sets/adversarial_asap\"\n","test_set_file = adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS.xls\"\n","\n","#verify data paths\n","print(test_set_file)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/StatNLP/FinalProject/Data Sets/adversarial_asap/valid_set_plus_ADVERSARIAL_ESSAYS.xls\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RZEoaPU6xJ7f","colab_type":"code","colab":{}},"source":["import random\n","# Attack 1: Shuffling Words\n","\n","#load excel into dataframe\n","test_set_shuffle = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_shuffle = test_set_shuffle.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_shuffle.index: \n","  words= test_set_shuffle.at[i, 'essay'].split()\n","  random.shuffle(words)\n","  new_sentence = ' '.join(words)\n","  test_set_shuffle.at[i,'essay'] = new_sentence  \n","  \n","test_set_shuffle.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SHUFFLE.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QSKe4wYxM7W","colab_type":"code","outputId":"137547c5-274f-47ca-d7d6-d07b6d065742","executionInfo":{"status":"ok","timestamp":1587540372732,"user_tz":300,"elapsed":1175,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!ls /content/drive/StatNLP/FinalProject/Data Sets"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ls: cannot access '/content/drive/StatNLP/FinalProject/Data': No such file or directory\n","ls: cannot access 'Sets': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gotmRA7Gx6gL","colab_type":"code","outputId":"a36fe048-04ee-48a1-edc5-72cba7ee6347","executionInfo":{"status":"ok","timestamp":1587540409074,"user_tz":300,"elapsed":1277,"user":{"displayName":"Dheeraj Kondaparthi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhKN5cRrlPtQfVXEjCmXRheG4iIhzDBq-YXmUur=s64","userId":"09317881797538835469"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["!ls /content/drive/StatNLP/FinalProject/Data\\ Sets/adversarial_asap"],"execution_count":0,"outputs":[{"output_type":"stream","text":["a\n","valid_sample_submission_5_column_plus_ADVERSARIAL_ESSAYS.csv\n","valid_set_plus_ADVERSARIAL_ESSAYS.xls\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WLeXsCoBx9S_","colab_type":"code","colab":{}},"source":["# Attack 2a: Appending - \"Library\"\n","\n","#load excel into dataframe\n","test_set_append = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_append = test_set_append.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_append.index: \n","  words= test_set_append.at[i, 'essay'].split()\n","  words.append(\"library\")\n","  new_sentence = ' '.join(words)\n","  test_set_append.at[i,'essay'] = new_sentence  \n","  \n","test_set_append.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-APPEND_LIBRARY.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ntq9wa_6yheP","colab_type":"code","colab":{}},"source":["# Attack 3a: Progressive Overload - \"Library\"\n","\n","#load excel into dataframe\n","test_set_progressive = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_progressive = test_set_progressive.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_progressive.index: \n","  words= test_set_progressive.at[i, 'essay'].split()\n","  if i < 591:\n","    continue\n","  if i < 641:\n","    for x in range(0,i-590):\n","      words[x] = \"library\"\n","  new_sentence = ' '.join(words)\n","  test_set_progressive.at[i,'essay'] = new_sentence  \n","  \n","test_set_progressive.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-PROGRESSIVE_LIBRARY.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WYCl-3rymiM","colab_type":"code","colab":{}},"source":["# Attack 4a: Single Substitution - \"Library\"\n","\n","#load excel into dataframe\n","test_set_single = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_single = test_set_single.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_single.index: \n","  words= test_set_single.at[i, 'essay'].split()\n","  if i < 591:\n","    continue\n","  if i < 641:\n","    words[i-591] = \"library\"\n","  new_sentence = ' '.join(words)\n","  test_set_single.at[i,'essay'] = new_sentence  \n","  \n","test_set_single.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SINGLE_LIBRARY.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzGC9JZjyqZR","colab_type":"code","colab":{}},"source":["# Attack 5a: Insertion of anchor in random locations\n","\n","#load excel into dataframe\n","test_set_insertion = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_insertion = test_set_insertion.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_insertion.index: \n","  words= test_set_insertion.at[i, 'essay'].split()\n","  x = random.randint(0,len(words))\n","  words.insert(x, 'library')\n","  new_sentence = ' '.join(words)\n","  test_set_insertion.at[i,'essay'] = new_sentence  \n","  \n","test_set_insertion.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-INSERTION_LIBRARY.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6YCo_PP2yyGu","colab_type":"code","colab":{}},"source":["# Attack 2b: Appending - \"Censorship\"\n","\n","#load excel into dataframe\n","test_set_append = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_append = test_set_append.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_append.index: \n","  words= test_set_append.at[i, 'essay'].split()\n","  words.append(\"censorship\")\n","  new_sentence = ' '.join(words)\n","  test_set_append.at[i,'essay'] = new_sentence  \n","  \n","test_set_append.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-APPEND_CENSORSHIP.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UIcDpIHOy2Zr","colab_type":"code","colab":{}},"source":["# Attack 3b: Progressive Overload - \"Censorship\"\n","\n","#load excel into dataframe\n","test_set_progressive = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_progressive = test_set_progressive.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_progressive.index: \n","  words= test_set_progressive.at[i, 'essay'].split()\n","  if i < 591:\n","    continue\n","  if i < 641:\n","    for x in range(0,i-590):\n","      words[x] = \"censorship\"\n","  new_sentence = ' '.join(words)\n","  test_set_progressive.at[i,'essay'] = new_sentence  \n","  \n","test_set_progressive.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-PROGRESSIVE_CENSORSHIP.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8MEohky2y6la","colab_type":"code","colab":{}},"source":["# Attack 4b: Single Substitution - \"Censorship\"\n","\n","#load excel into dataframe\n","test_set_single = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_single = test_set_single.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_single.index: \n","  words= test_set_single.at[i, 'essay'].split()\n","  if i < 591:\n","    continue\n","  if i < 641:\n","    words[i-591] = \"censorship\"\n","  new_sentence = ' '.join(words)\n","  test_set_single.at[i,'essay'] = new_sentence  \n","  \n","test_set_single.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SINGLE_CENSORSHIP.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yUH5BUd5y-q4","colab_type":"code","colab":{}},"source":["# Attack 5b: Insertion of \"censorship\" in random locations\n","\n","#load excel into dataframe\n","test_set_insertion = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_insertion = test_set_insertion.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_insertion.index: \n","  words= test_set_insertion.at[i, 'essay'].split()\n","  x = random.randint(0,len(words))\n","  words.insert(x, 'censorship')\n","  new_sentence = ' '.join(words)\n","  test_set_insertion.at[i,'essay'] = new_sentence  \n","  \n","test_set_insertion.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-INSERTION_CENSORSHIP.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BmW8ruCkzCbO","colab_type":"code","colab":{}},"source":["# Attack 2c: Appending - \"The\"\n","\n","#load excel into dataframe\n","test_set_append = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_append = test_set_append.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_append.index: \n","  words= test_set_append.at[i, 'essay'].split()\n","  words.append(\"the\")\n","  new_sentence = ' '.join(words)\n","  test_set_append.at[i,'essay'] = new_sentence  \n","  \n","test_set_append.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-APPEND_THE.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XB36QB_hzFvq","colab_type":"code","colab":{}},"source":["# Attack 3c: Progressive Overload - \"The\"\n","\n","#load excel into dataframe\n","test_set_progressive = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_progressive = test_set_progressive.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_progressive.index: \n","  words= test_set_progressive.at[i, 'essay'].split()\n","  if i < 591:\n","    continue\n","  if i < 641:\n","    for x in range(0,i-590):\n","      words[x] = \"the\"\n","  new_sentence = ' '.join(words)\n","  test_set_progressive.at[i,'essay'] = new_sentence  \n","  \n","test_set_progressive.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-PROGRESSIVE_THE.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2S0Bo4vFzIuD","colab_type":"code","colab":{}},"source":["# Attack 4c: Single Substitution - \"The\"\n","\n","#load excel into dataframe\n","test_set_single = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_single = test_set_single.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_single.index: \n","  words= test_set_single.at[i, 'essay'].split()\n","  if i < 591:\n","    continue\n","  if i < 641:\n","    words[i-591] = \"censorship\"\n","  new_sentence = ' '.join(words)\n","  test_set_single.at[i,'essay'] = new_sentence  \n","  \n","test_set_single.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-SINGLE_THE.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ShHpiW1zL6w","colab_type":"code","colab":{}},"source":["# Attack 5c: Insertion of \"the\" in random locations\n","\n","#load excel into dataframe\n","test_set_insertion = pd.read_excel(test_set_file, sheet_name='valid_set')\n","\n","#remove empty n/a cells\n","test_set_insertion = test_set_insertion.drop(['domain2_predictionid'], axis=1)    \n","\n","\n","for i in test_set_insertion.index: \n","  words= test_set_insertion.at[i, 'essay'].split()\n","  x = random.randint(0,len(words))\n","  words.insert(x, 'the')\n","  new_sentence = ' '.join(words)\n","  test_set_insertion.at[i,'essay'] = new_sentence  \n","  \n","test_set_insertion.to_excel(adversarial_dir+\"/valid_set_plus_ADVERSARIAL_ESSAYS-INSERTION_THE.xls\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R2hx0MgbzO2V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}